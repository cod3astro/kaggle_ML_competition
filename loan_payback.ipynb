{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cod3astro/kaggle_ML_competition/blob/main/loan_payback.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sv4ODzDEx3C",
        "outputId": "83be5138-48d7-47fa-ad40-e7eff0a4e623"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYlTxouGo-G7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQLEq93p_1TJ"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('train.csv', index_col='id')\n",
        "target = train_df['loan_paid_back']\n",
        "train_df.drop(columns=['loan_paid_back'], inplace=True)\n",
        "train_df['is_train'] = 1 # Mark training data\n",
        "\n",
        "test_df = pd.read_csv('test.csv', index_col='id')\n",
        "test_df['is_train'] = 0 # Mark test data\n",
        "\n",
        "df = pd.concat([train_df, test_df])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "jPt4raDrQubp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().any().any()"
      ],
      "metadata": {
        "id": "HL_CDsOnQwjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6B0iWAWAXyF"
      },
      "outputs": [],
      "source": [
        "df.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80557a80"
      },
      "source": [
        "numerical_cols = df.select_dtypes(include=np.number).columns\n",
        "skewness = df[numerical_cols].skew().sort_values(ascending=False)\n",
        "\n",
        "print(\"Skewness of numerical features:\\n\")\n",
        "print(skewness)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58195483"
      },
      "source": [
        "Let's visualize the distribution of the top 5 most skewed numerical features to better understand their skewness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "549c89c2"
      },
      "source": [
        "top_skewed_features = skewness.head(2).index\n",
        "bottom_skewed_features = skewness.tail(2).index\n",
        "\n",
        "def plot_skewness(columns):\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    for i, col in enumerate(columns):\n",
        "        plt.subplot(2, 3, i + 1)\n",
        "        sns.histplot(df[col], kde=True)\n",
        "        plt.title(f'Distribution of {col} (Skewness: {skewness[col]:.2f})')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('Frequency')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "plot_skewness(columns=top_skewed_features)\n",
        "plot_skewness(columns=bottom_skewed_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c59c98e6"
      },
      "source": [
        "for col in ['annual_income', 'debt_to_income_ratio']:\n",
        "    df[col] = np.log1p(df[col])\n",
        "    print(f'Transformed skewness of {col}: {df[col].skew():.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3d13b4f"
      },
      "source": [
        "transformed_features = ['annual_income', 'debt_to_income_ratio']\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "for i, col in enumerate(transformed_features):\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    sns.histplot(df[col], kde=True)\n",
        "    plt.title(f'Distribution of Transformed {col} (Skewness: {df[col].skew():.2f})')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdcc462d"
      },
      "source": [
        "# Reload df to revert previous transformations on 'debt_to_income_ratio'\n",
        "train = pd.read_csv('train.csv', index_col='id')\n",
        "train.drop(columns=['loan_paid_back'], inplace=True)\n",
        "train['is_train'] = 1\n",
        "\n",
        "test = pd.read_csv('test.csv', index_col='id')\n",
        "test['is_train'] = 0\n",
        "\n",
        "df = pd.concat([train, test])\n",
        "\n",
        "# Apply log1p transformation to 'annual_income'\n",
        "df['annual_income'] = np.log1p(df['annual_income'])\n",
        "print(f\"Transformed skewness of annual_income (log1p): {df['annual_income'].skew():.2f}\")\n",
        "\n",
        "# Apply square root transformation to 'debt_to_income_ratio'\n",
        "df['debt_to_income_ratio'] = np.sqrt(df['debt_to_income_ratio'])\n",
        "print(f\"Transformed skewness of debt_to_income_ratio (sqrt): {df['debt_to_income_ratio'].skew():.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77d9afe0"
      },
      "source": [
        "from scipy.stats import boxcox\n",
        "\n",
        "# Reload df to revert previous transformations on 'debt_to_income_ratio'\n",
        "train = pd.read_csv('train.csv', index_col='id')\n",
        "train.drop(columns=['loan_paid_back'], inplace=True)\n",
        "train['is_train'] = 1\n",
        "\n",
        "test = pd.read_csv('test.csv', index_col='id')\n",
        "test['is_train'] = 0\n",
        "\n",
        "df = pd.concat([train, test])\n",
        "\n",
        "# Apply Box-Cox transformation to 'debt_to_income_ratio'\n",
        "# Box-Cox requires data to be strictly positive. Check and ensure.\n",
        "for col in ['debt_to_income_ratio', 'annual_income']:\n",
        "    if (df[col] <= 0).any():\n",
        "        print(\"Warning:\", col, \"contains non-positive values. Box-Cox might not be suitable or requires adjustment.\")\n",
        "        # A common approach for Box-Cox with non-positive values is to add a small constant\n",
        "        # df['debt_to_income_ratio_transformed'], lambda_val = boxcox(df['debt_to_income_ratio'] + 1e-6)\n",
        "    else:\n",
        "        df[col], lambda_val = boxcox(df[col])\n",
        "        print(f\"Transformed skewness of debt_to_income_ratio (Box-Cox, lambda={lambda_val:.2f}): {df['debt_to_income_ratio'].skew():.2f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ffc6d83"
      },
      "source": [
        "transformed_features = ['annual_income', 'debt_to_income_ratio']\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "for i, col in enumerate(transformed_features):\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    sns.histplot(df[col], kde=True)\n",
        "    plt.title(f'Distribution of Transformed {col} (Skewness: {df[col].skew():.2f})')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_col = df.select_dtypes(include='object').columns\n",
        "for col in categorical_col:\n",
        "    unique_values = df[col].unique()\n",
        "    print(f'{col} ({len(unique_values)} unique)')\n",
        "    print(df[col].unique())"
      ],
      "metadata": {
        "id": "MpXpJ_JmZ-0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea2983cc"
      },
      "source": [
        "train = df[df['is_train'] == 1].copy()\n",
        "train.drop(columns=['is_train'], inplace=True)\n",
        "\n",
        "display(train.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.shape)\n",
        "print(train_df.shape)"
      ],
      "metadata": {
        "id": "ydSfCbLOgqji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.merge(target, left_index=True, right_index=True)"
      ],
      "metadata": {
        "id": "r9za3evXkOiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "7m6pYLd4miIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['loan_paid_back'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "MPi9WrlQmz8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.impute import SimpleImputer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.stats import pointbiserialr, chi2_contingency\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline"
      ],
      "metadata": {
        "id": "paaGvuk2mBdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train.drop(columns=['loan_paid_back'])\n",
        "y = train['loan_paid_back']"
      ],
      "metadata": {
        "id": "CaSZfP-VoFNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_col = X.select_dtypes(include=np.number).columns\n",
        "\n",
        "def cramers_v(x, y):\n",
        "    confusion_matrix = pd.crosstab(x, y)\n",
        "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "    n = confusion_matrix.sum().sum()\n",
        "    phi2 = chi2 / n\n",
        "    r, k = confusion_matrix.shape\n",
        "    phi2corr = max(0, phi2 - ((k-1)*(r-1)) / (n-1))\n",
        "    rcorr = r - ((r-1)**2)/(n-1)\n",
        "    kcorr = k - ((k-1)**2)/(n-1)\n",
        "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
        "\n",
        "cramers_scores = {}\n",
        "for col in categorical_col:\n",
        "    cramers_scores[col] = cramers_v(train[col], train['loan_paid_back'])\n",
        "\n",
        "X_num = train[numerical_col]\n",
        "y = train['loan_paid_back']\n",
        "mi_scores = mutual_info_classif(X_num, y, discrete_features=False)\n",
        "mi_scores_dict = dict(zip(numerical_col, mi_scores))\n",
        "\n",
        "all_scores = {**cramers_scores, **mi_scores_dict}\n",
        "all_scores_series = pd.Series(all_scores).sort_values(ascending=False)\n",
        "\n",
        "print(\"\\nüìä Combined Feature Correlation Report with Target:\")\n",
        "print(all_scores_series)"
      ],
      "metadata": {
        "id": "BKvK9eDclzHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=42)"
      ],
      "metadata": {
        "id": "gKLFBmtFrJqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onehot_col = ['gender', 'marital_status', 'employment_status', 'loan_purpose']\n",
        "ordinal_col = ['education_level', 'grade_subgrade']"
      ],
      "metadata": {
        "id": "n5jlNqfurZTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define ordinal categories based on their intrinsic order\n",
        "education_categories = ['Other', 'High School', \"Bachelor's\", \"Master's\", 'PhD']\n",
        "\n",
        "# Generate grade_subgrade categories in the correct order (A1-F5)\n",
        "grades = ['A', 'B', 'C', 'D', 'E', 'F']\n",
        "subgrades = [str(i) for i in range(1, 6)]\n",
        "grade_subgrade_categories = [g + s for g in grades for s in subgrades]\n",
        "\n",
        "# Preprocessing steps\n",
        "numerical_transformer = StandardScaler()\n",
        "onehot_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "ordinal_transformer = OrdinalEncoder(categories=[education_categories, grade_subgrade_categories], handle_unknown='use_encoded_value', unknown_value=-1) # handle_unknown='use_encoded_value' and unknown_value=-1 for unseen categories\n",
        "\n",
        "# Create a column transformer to apply different transformations to different columns\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_col),\n",
        "        ('onehot', onehot_transformer, onehot_col),\n",
        "        ('ordinal', ordinal_transformer, ordinal_col)\n",
        "    ],\n",
        "    remainder='passthrough' # Keep other columns (like 'is_train' if present, though it's dropped from X)\n",
        ")\n",
        "\n",
        "# Create an imblearn pipeline that includes preprocessing, SMOTE, and a classifier\n",
        "# SMOTE will be applied only on training data within each CV fold\n",
        "pipeline_ = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', LogisticRegression(solver='liblinear', random_state=42)) # Placeholder classifier\n",
        "])\n",
        "\n",
        "print(\"Preprocessing pipeline_ with SMOTE defined successfully.\")\n",
        "print(pipeline_)"
      ],
      "metadata": {
        "id": "4PLj6GLMEJGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac70d451"
      },
      "source": [
        "education_categories = ['Other', 'High School', \"Bachelor's\", \"Master's\", 'PhD']\n",
        "\n",
        "def sort_grade_subgrade(grades):\n",
        "    # Custom sort key for grade_subgrade\n",
        "    def get_sort_key(item):\n",
        "        grade = item[0]\n",
        "        subgrade = int(item[1:])\n",
        "        return (grade, subgrade)\n",
        "    return sorted(grades, key=get_sort_key)\n",
        "\n",
        "grade_subgrade_categories = sort_grade_subgrade(df['grade_subgrade'].unique())\n",
        "\n",
        "print(\"Education Categories:\", education_categories)\n",
        "print(\"Grade Subgrade Categories (first 10):\", grade_subgrade_categories[:10])\n",
        "print(\"Grade Subgrade Categories (last 10):\", grade_subgrade_categories[-10:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3812b922"
      },
      "source": [
        "# Instantiate base models with random_state=42 where applicable\n",
        "logreg_model = LogisticRegression(solver='liblinear', random_state=42)\n",
        "decision_model = DecisionTreeClassifier(random_state=42)\n",
        "random_model = RandomForestClassifier(random_state=42)\n",
        "gradient_model = GradientBoostingClassifier(random_state=42)\n",
        "svc_model = SVC(random_state=42, probability=True) # Added probability=True for roc_auc\n",
        "knn_model = KNeighborsClassifier()\n",
        "xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "lgbm_model = LGBMClassifier(random_state=42, verbosity=-1) # Added verbosity to suppress warnings\n",
        "catboost_model = CatBoostClassifier(random_state=42, verbose=0)\n",
        "\n",
        "# Create a dictionary to map model names to their objects\n",
        "models = {\n",
        "    'LogisticRegression': logreg_model,\n",
        "    'DecisionTree': decision_model,\n",
        "    'RandomForest': random_model,\n",
        "    'XGBoost': xgb_model,\n",
        "    'CatBoost': catboost_model,\n",
        "    'LightGBM': lgbm_model,\n",
        "    'GradientBoosting': gradient_model,\n",
        "    'SVC': svc_model,\n",
        "    'KNN': knn_model\n",
        "}\n",
        "# Define parameter grids for each model, prefixed with 'classifier__'\n",
        "param_grids = {\n",
        "    'LogisticRegression': {\n",
        "        'classifier__C': [0.01, 0.1, 1],\n",
        "        'classifier__penalty': ['l1', 'l2'],\n",
        "        'classifier__solver': ['liblinear']\n",
        "    },\n",
        "    'DecisionTree': {\n",
        "        'classifier__criterion': ['gini', 'entropy'],\n",
        "        'classifier__max_depth': [5, 10, 15, 20],\n",
        "        'classifier__min_samples_split': [2, 5, 10],\n",
        "        'classifier__min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'RandomForest': {\n",
        "        'classifier__n_estimators': [100, 200, 300],\n",
        "        'classifier__max_depth': [5, 10, 15],\n",
        "        'classifier__min_samples_split': [2, 5, 10],\n",
        "        'classifier__min_samples_leaf': [1, 2, 4],\n",
        "        'classifier__max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'classifier__n_estimators': [100, 200],\n",
        "        'classifier__learning_rate': [0.01, 0.05, 0.1],\n",
        "        'classifier__max_depth': [3, 5],\n",
        "        'classifier__subsample': [0.7, 1.0],\n",
        "        'classifier__colsample_bytree': [0.7, 1.0]\n",
        "    },\n",
        "    'CatBoost': {\n",
        "        'classifier__iterations': [100, 200],\n",
        "        'classifier__learning_rate': [0.05, 0.1],\n",
        "        'classifier__depth': [4, 6, 8],\n",
        "        'classifier__l2_leaf_reg': [1, 3, 5]\n",
        "    },\n",
        "    'LightGBM': {\n",
        "        'classifier__n_estimators': [100, 200, 400],\n",
        "        'classifier__learning_rate': [0.01, 0.05, 0.1],\n",
        "        'classifier__num_leaves': [20, 31, 40],\n",
        "        'classifier__subsample': [0.7, 1.0],\n",
        "        'classifier__colsample_bytree': [0.7, 1.0]\n",
        "    },\n",
        "    'GradientBoosting': {\n",
        "        'classifier__n_estimators': [100, 200, 500],\n",
        "        'classifier__learning_rate': [0.01, 0.05, 0.1],\n",
        "        'classifier__max_depth': [3, 5, 7],\n",
        "        'classifier__subsample': [0.7, 1.0]\n",
        "    },\n",
        "    'SVC': {\n",
        "        'classifier__C': [0.1, 1, 10],\n",
        "        'classifier__kernel': ['linear', 'rbf'],\n",
        "        'classifier__gamma': ['scale', 'auto', 0.1, 1]\n",
        "    },\n",
        "    'KNN': {\n",
        "        'classifier__n_neighbors': [3, 5, 7],\n",
        "        'classifier__weights': ['uniform', 'distance'],\n",
        "        'classifier__metric': ['euclidean', 'manhattan']\n",
        "    }\n",
        "}\n",
        "print(\"Defined models and their parameter grids:\")\n",
        "for model_name, params in param_grids.items():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    for param_name, values in params.items():\n",
        "        print(f\"  {param_name}: {values}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b3f19f6"
      },
      "source": [
        "results = {}\n",
        "\n",
        "# Iterate through the 'models' dictionary\n",
        "for model_name, model_object in models.items():\n",
        "\n",
        "    print(f\"--- Starting GridSearchCV for: {model_name} ---\")\n",
        "\n",
        "    # Create the pipeline *inside* the loop\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('smote', SMOTE(random_state=42)),\n",
        "        ('classifier', model_object)  # The model object\n",
        "    ])\n",
        "\n",
        "    # Get the correct parameter grid from param_grids\n",
        "    current_param_grid = param_grids[model_name]\n",
        "\n",
        "    # Instantiate GridSearchCV\n",
        "    model_grid_search = GridSearchCV(\n",
        "        pipeline,           # Use the pipeline\n",
        "        current_param_grid, # Use the model's specific grid\n",
        "        scoring='roc_auc',\n",
        "        cv=5,\n",
        "        n_jobs=-1,          # Use all available cores\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Fit the grid search *INSIDE* the loop\n",
        "    model_grid_search.fit(train_X, train_y)\n",
        "\n",
        "    # Print results *INSIDE* the loop\n",
        "    print(f\"\\nBest ROC AUC Score for {model_name}: {model_grid_search.best_score_:.4f}\")\n",
        "    print(f\"Best Parameters for {model_name}: {model_grid_search.best_params_}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Store results\n",
        "    results[model_name] = {\n",
        "        'best_score': model_grid_search.best_score_,\n",
        "        'best_params': model_grid_search.best_params_,\n",
        "        'best_estimator': model_grid_search.best_estimator_\n",
        "    }\n",
        "\n",
        "print(\"\\n=== All Grid Searches Complete ===\")\n",
        "# You can now inspect the 'results' dictionary\n",
        "for model_name, result in results.items():\n",
        "    print(f\"{model_name} Best Score: {result['best_score']:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVbIz4oEa+nknqi8mbppFr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}